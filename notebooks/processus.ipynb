{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Partial credits for this code goes to : https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from models.LeNet import LeNet\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.optim import Adam\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training hyperparameters\n",
        "INIT_LR = 1e-3\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "# Define the train and val splits\n",
        "TRAIN_SPLIT = 0.75\n",
        "VAL_SPLIT = 1 - TRAIN_SPLIT\n",
        "\n",
        "# Configure the device we will be using to train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Argument parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Construct the argument parser and parse the arguments\n",
        "#ap = argparse.ArgumentParser()\n",
        "#ap.add_argument(\"-m\", \"--model\", type=str, required=True, help=\"path to output trained model\")\n",
        "#ap.add_argument(\"-p\", \"--plot\", type=str, required=True, help=\"path to output loss/accuracy plot\")\n",
        "#args = vars(ap.parse_args())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading dataset...\n",
            "[INFO] generating the train/validation split...\n"
          ]
        }
      ],
      "source": [
        "print(\"[INFO] loading dataset...\")\n",
        "trainData = MNIST(root=\"../data\", train=True, download=True, transform=ToTensor())\n",
        "testData = MNIST(root=\"../data\", train=False, download=True, transform=ToTensor())\n",
        "\n",
        "# Calculate the train/validation split\n",
        "print(\"[INFO] generating the train/validation split...\")\n",
        "numTrainSamples = int(len(trainData) * TRAIN_SPLIT)\n",
        "numValSamples = int(len(trainData) * VAL_SPLIT)\n",
        "(trainData, valData) = random_split(trainData, [numTrainSamples, numValSamples], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the train, validation, and test data loaders\n",
        "trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=BATCH_SIZE)\n",
        "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE)\n",
        "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Calculate steps per epoch for training and validation set\n",
        "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
        "valSteps = len(valDataLoader.dataset) // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] initializing the LeNet model...\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "print(\"[INFO] initializing the LeNet model...\")\n",
        "model = LeNet(numChannels=1, classes=len(trainData.dataset.classes)).to(device)\n",
        "\n",
        "# Initialize our optimizer and loss function\n",
        "opt = Adam(model.parameters(), lr=INIT_LR)\n",
        "lossFn = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (64x1568 and 800x500)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m (x, y) \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Perform a forward pass and calculate the training loss\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossFn(pred, y)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Zero out the gradients, perform the backpropagation step, and update the weights\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\fwilh\\.conda\\envs\\GTI755\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\fwilh\\.conda\\envs\\GTI755\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\fwilh\\Desktop\\GTI755\\src\\models\\LeNet.py:45\u001b[0m, in \u001b[0;36mLeNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m flatten(x, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten the output from the previous layer and pass it\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# ==== FC layer\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu3(x)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# ==== Softmax classifier\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\fwilh\\.conda\\envs\\GTI755\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\fwilh\\.conda\\envs\\GTI755\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\fwilh\\.conda\\envs\\GTI755\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x1568 and 800x500)"
          ]
        }
      ],
      "source": [
        "# loop over our epochs\n",
        "for e in range(0, EPOCHS):\n",
        "\t# Set the model in training mode\n",
        "\tmodel.train()\n",
        "\t# Initialize the total training/validation loss/correct predictions\n",
        "\ttotalTrainLoss, totalValLoss, trainCorrect, valCorrect = 0, 0, 0, 0\n",
        "\t\n",
        "\t# Training\n",
        "\tfor (x, y) in trainDataLoader:\n",
        "\t\t# Send the input to the device\n",
        "\t\t(x, y) = (x.to(device), y.to(device))\n",
        "\t\t# Perform a forward pass and calculate the training loss\n",
        "\t\tpred = model(x)\n",
        "\t\tloss = lossFn(pred, y)\n",
        "\t\t# Zero out the gradients, perform the backpropagation step, and update the weights\n",
        "\t\topt.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\topt.step()\n",
        "\t\t# Sum the loss to the total training loss so far\n",
        "\t\ttotalTrainLoss += loss\n",
        "\t\t# Calculate the number of correct predictions\n",
        "\t\ttrainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\t\n",
        "\t# Validation\n",
        "\twith torch.no_grad():\n",
        "\t\t# Set the model in evaluation mode\n",
        "\t\tmodel.eval()\n",
        "\t\t# Loop over the validation set\n",
        "\t\tfor (x, y) in valDataLoader:\n",
        "\t\t\t# Send the input to the device\n",
        "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
        "\t\t\t# Make the predictions and calculate the validation loss\n",
        "\t\t\tpred = model(x)\n",
        "\t\t\ttotalValLoss += lossFn(pred, y)\n",
        "\t\t\t# Calculate the number of correct predictions\n",
        "\t\t\tvalCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "\t# Calculate the average training and validation loss\n",
        "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
        "\tavgValLoss = totalValLoss / valSteps\n",
        "\t# Calculate the training and validation accuracy\n",
        "\ttrainCorrect = trainCorrect / len(trainDataLoader.dataset)\n",
        "\tvalCorrect = valCorrect / len(valDataLoader.dataset)\n",
        "\n",
        "\t# Update our training history\n",
        "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "\tH[\"train_acc\"].append(trainCorrect)\n",
        "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "\tH[\"val_acc\"].append(valCorrect)\n",
        "\t# Print the model training and validation information\n",
        "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
        "\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n",
        "\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(avgValLoss, valCorrect))\n",
        "\t\n",
        "# finish measuring how long training took\n",
        "endTime = time.time()\n",
        "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We can now evaluate the network on the test set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "\n",
        "with torch.no_grad(): # Turn off autograd for testing evaluation\n",
        "\tmodel.eval() # Set the model in evaluation mode\n",
        "\tpreds = [] # Initialize a list to store our predictions\n",
        "\t# Evaluation with test dataset\n",
        "\tfor (x, y) in testDataLoader:\n",
        "\t\t# Send the input to the device\n",
        "\t\tx = x.to(device)\n",
        "\t\t# Make the predictions and add them to the list\n",
        "\t\tpred = model(x)\n",
        "\t\tpreds.extend(pred.argmax(axis=1).cpu().numpy())\n",
        "\t\t\n",
        "# Generate a classification report\n",
        "print(classification_report(testData.targets.cpu().numpy(), np.array(preds), target_names=testData.classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
        "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(args[\"plot\"])\n",
        "\n",
        "# Serialize the model to disk\n",
        "torch.save(model, args[\"model\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
